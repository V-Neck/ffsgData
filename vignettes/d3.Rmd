---
title: "Linkage Analysis"
author: "Vaughn Johnson"
date: "5/17/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(purrr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(venn)
library(igraph)

path_to_src = here::here(file.path('R'))

load(file=file.path(path_to_src,
                         "FinalClassification",
                         "full_classification.RData"))

load(file=file.path(path_to_src,
                         "FinalClassification",
                         "full_combined_harmonized.RData"))
```

```{r}
link_idx = classification$prediction == 'L'
links = classification$pairs[link_idx, c('id1', 'id2')]
link_graph = graph_from_edgelist(as.matrix(links), directed=F)
linked_ids = groups(components(link_graph, mode="weak"))
linked_ids = linked_ids[unlist(lapply(
                            linked_ids, function(x) length(x) > 1))]

linked_sources = lapply(linked_ids, 
                        function(x)  combined_harmonized[x, 'source'])

dup_link_idx = unlist(lapply(linked_sources, function(x) any(duplicated(x))))

dup_links      =    linked_ids[ dup_link_idx]
unique_links   = linked_ids[!dup_link_idx]
unique_sources = linked_sources[!dup_link_idx]
```

There are literally only 448 records that are duplicated within one database.
I'm just going to ignore those and move on


```{r}
load(file=file.path(path_to_src,
                         "FinalClassification",
                         "full_combined_harmonized.RData"))

sources = unique(combined_harmonized[, 'source'])

for (source in sources) {
    col_name = paste0("in_", source)
    combined_harmonized[col_name] = combined_harmonized[, 'source'] == source
}

collaps_vals = function(x) {
    return(x[!is.na(x)][1])
}

end = length(unique_links)
idx = 1
drop_rows = c()

for (link in unique_links) {
    idx = idx + 1
    if (idx %% 50 == 0) {
        print(idx / end)
    }
    keep_row_idx = link[1]
    drop_rows = c(drop_rows, link[2:length(link)])
    link_source = combined_harmonized[link, 'source']
    
    for (source in sources) {
        col_name = paste0("in_", source)
        combined_harmonized[keep_row_idx, col_name] = source %in% link_source
    }
    rows = combined_harmonized[link, ]
    row = lapply(rows, collaps_vals)
    
    combined_harmonized[keep_row_idx, ] = row
}

combined_harmonized = combined_harmonized[-drop_rows, ] %>%
                        select(-c("source"))
```

```{r}
venn( 1 * combined_harmonized[, c('fe', 'wapo', 'kbp', 'mpv')])
```

```{r}
f = file("~/Desktop/police_shootings.csv", encoding="UTF-8")
write.csv(combined_harmonized, f)
```

# Collapse Duplicate rows
```{r}
load(file=file.path(path_to_src,
                         "FinalClassification",
                         "full_combined_harmonized.RData"))

collaps_vals = function(x) {
    return(x[!is.na(x)][1])
}

# This would break if the max row wasn't linked to anything
combined_harmonized['component'] = components(link_graph, mode="weak")$membership

x = combined_harmonized %>% 
    group_by(component) %>%
    spread(source, source) %>%
    summarise_all(collaps_vals)

con = file('~/Desktop/police_shootings', encoding="UTF-8")
write.csv(x, file=con)
```

```{r}
y = x[, c('fe', 'wapo', 'kbp', 'mpv', 'year')]
y = data.frame(!is.na(y %>% filter(year > 2015) %>% select(c('fe', 'wapo', 'kbp', 'mpv'))))

venn(y, zcolor=c("black", "black", "black", "black"), opacity=.1)
```

```{r}
load(file=file.path(path_to_src,
                         "ScrapedFiles",
                         "Populations",
                         "CountyPop.RData"))

states = pop_county %>% 
    filter(county_num == 0) %>%
    select(c('state_abb', pop_estimate_cols)) %>%
    gather('year', 'population', pop_estimate_cols) %>%
    mutate(year = gsub(".*(20[0-9]+)", "\\1", year))

pop_estimate_cols = colnames(states)[grepl("POPESTIMATE", colnames(states))]

# This would break if the max row wasn't linked to anything
combined_harmonized['component'] = components(link_graph, mode="weak")$membership

final_combo = combined_harmonized %>% 
    group_by(component) %>%
    spread(source, source) %>%
    summarise_all(collaps_vals)

con = file('~/Desktop/police_shootings.csv', encoding="UTF-8")
write.csv(final_combo, file=con)


con = file('~/Desktop/state_pops.csv', encoding="UTF-8")
write.csv(states, file=con)

library(jsonlite)
states %>% toJSON() %>% write_lines("~/Desktop/state_pops.json")
```
